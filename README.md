# Live2Dアニメキャラクター×AIチャット アプリ

このプロジェクトは、Live2DアニメキャラクターとOpenAI(ChatGPT)APIを組み合わせたインタラクティブなWebアプリケーションです。ユーザーがテキストを入力すると、キャラクターが表情豊かに反応し、AIの返答を音声で読み上げます。

## 特徴

- Live2Dキャラクターの表示とアニメーション
- ChatGPT APIを使用したインテリジェントな会話
- テキスト読み上げによる音声応答
- リップシンク（音声に合わせた口の動き）機能
- 感情分析に基づくキャラクターの表情変化

## 技術スタック

### フロントエンド
- HTML5/CSS3/JavaScript
- [Live2D Cubism SDK for Web](https://www.live2d.com/en/download/cubism-sdk/download-web/)
- [pixi-live2d-display](https://github.com/guansss/pixi-live2d-display)（Pixi.js上でのLive2D表示用）
- Web Audio API（音声再生・リップシンク用）

### バックエンド
- Node.js + Express（サーバーサイド処理）
- OpenAI API（ChatGPT 3.5/4）
- 音声合成サービス
  - [VOICEVOX](https://voicevox.hiroshiba.jp/)（無料の音声合成エンジン）
  - Azure Cognitive Services（オプション）

## セットアップ

### 前提条件
- Node.js (バージョン14以上)
- npm または yarn
- OpenAI APIキー
- VOICEVOX（無料の音声合成エンジン）をローカルで起動するか、Azure Cognitive Servicesのキー

### インストール手順
1. リポジトリをクローン:
```
git clone https://github.com/mamama6147/live2d-ai-chat.git
cd live2d-ai-chat
```

2. 依存関係のインストール:
```
# クライアント側
cd client
npm install

# サーバー側
cd ../server
npm install
```

3. 環境変数の設定:
サーバーディレクトリに `.env` ファイルを作成し、以下のように設定:
```
OPENAI_API_KEY=your_openai_api_key
TTS_SERVICE=voicevox
VOICEVOX_ENDPOINT=http://localhost:50021
VOICEVOX_SPEAKER_ID=3  # ずんだもん（ノーマル）
```

4. Live2Dモデルの配置:
公式サンプルまたは自作モデルを `client/public/models` ディレクトリに配置します。

### VOICEVOXの設定

このプロジェクトはデフォルトでVOICEVOXを使用します。VOICEVOXは無料の日本語音声合成エンジンで、様々なキャラクターボイスが利用可能です。

1. [VOICEVOX公式サイト](https://voicevox.hiroshiba.jp/)からソフトウェアをダウンロード・インストール
2. VOICEVOXを起動し、バックグラウンドで実行したままにする
3. デフォルトでは `http://localhost:50021` でサービスが提供されます

#### サポートされているVOICEVOX話者

以下のIDで多数の話者とスタイルをサポートしています：
- 四国めたん: 2（ノーマル）、0（あまあま）、6（ツンツン）など
- ずんだもん: 3（ノーマル）、1（あまあま）、7（ツンツン）など
- その他多数の話者

詳細はクライアント設定UIから選択するか、VOICEVOXのソフトウェアで確認できます。

## 開発環境の起動

```
# サーバー起動 (server ディレクトリ内で)
npm run dev

# クライアント開発サーバー起動 (client ディレクトリ内で)
npm run dev
```

## Live2Dモデルについて

### モデルのリップシンクパラメータ

Live2Dモデルのリップシンク（口パク）機能を使用するには、モデルが適切なパラメータを持っている必要があります。このプロジェクトでは、以下のようなパラメータ名を自動的に検出して口の動きに使用します：

- 一般的なLive2Dパラメータ
  - `ParamMouthOpenY`
  - `PARAM_MOUTH_OPEN_Y`
  - `ParamMouthOpen`
  - `PARAM_MOUTH_OPEN`

- 虹色まおモデル用パラメータ
  - `ParamA` (虹色まおのmodel3.jsonでLipSyncグループに指定されているパラメータ)

モデルによって使用されるパラメータが異なる場合は、`client/main.js`の以下の部分を修正してください：

```javascript
// 口の開閉値を複数のパラメータに適用する関数
function applyMouthOpenValue(value) {
  // ...
  const mouthParams = [
    'ParamA',             // 虹色まおの口パクパラメータ
    'ParamMouthOpenY',
    'PARAM_MOUTH_OPEN_Y',
    // 必要に応じて他のパラメータを追加
  ];
  // ...
}
```

### 口の開き具合の設定

キャラクターの口の開き具合は、`client/main.js`の`applyMouthOpenValue`関数内で調整できます。デフォルトでは2.0倍に増幅されています（v1.1より変更）。

```javascript
// 口の開きを大きくするために値を増幅（2.0倍に増幅）
const amplifiedValue = Math.min(value * 2.0, 1);
```

この値を変更することで、口の開き具合を調整できます。ただし、最大値は1.0なので、どんなに大きく設定しても`Math.min`関数により1.0を超えることはありません。

## リップシンク機能について

リップシンク機能はWeb Audio APIを利用して音声の音量を分析し、それに応じてキャラクターの口の動きをリアルタイムで制御します。この機能には3つのモードがあります：

- **自動**: 音声ファイルが正常に再生された場合は音声解析ベースのリップシンク、失敗した場合はダミーリップシンクにフォールバック
- **音声解析**: 音声のフーリエ変換を使用してリアルタイムに口の開閉を制御
- **ダミー**: ランダムノイズを用いた簡易的な口パク動作（音声ファイルが無い場合やデバッグ用）
- **オフ**: リップシンク機能を無効化

この機能はクライアント側の設定UIから切り替えることができます。

### リップシンク機能の改善点（v1.1）

最新のアップデートでは、リップシンク機能に以下の改善を加えました：

1. **反応速度の向上**: スムージング係数を0.3から0.5に増加させ、口の動きが音声により速く追従するようにしました
2. **解析精度の向上**: FFTサイズを256から512に増加させ、音声解析の精度を向上させました
3. **口の開閉度の強調**: 口の開き具合を1.5倍から2.0倍に増幅し、よりはっきりと口の動きが見えるようにしました
4. **フレームレートの向上**: ダミーリップシンクのアップデート間隔を50msから33msに短縮し、より滑らかな動きを実現しました
5. **人間の声に合わせた分析**: 音声解析時に人間の声の周波数帯（約80Hz～3000Hz）に重点を置くように調整しました

これらの改善により、音声に合わせた口の動きがより自然で明確になりました。

## 機能実装ステータス

- [x] プロジェクト構造のセットアップ
- [x] Live2Dモデル表示
- [x] ChatGPT API連携
- [x] 音声合成機能（VOICEVOX/Azure）
- [x] リップシンク実装
- [x] 表情変化の実装
- [x] チャットUI

## 変更履歴

### v1.1
- リップシンク機能の改善（反応速度・精度・表現力の向上）
- 音声解析アルゴリズムの調整
- 口の動きの増幅率を増加

### v1.0
- 初期リリース

## ライセンス

このプロジェクトは開発中です。Live2Dモデルを含む各コンポーネントのライセンスに従ってください。